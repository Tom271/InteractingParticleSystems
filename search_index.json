[
["index.html", "A Numerical Study of an Interacting Particle System End of Year 1 Report Preface", " A Numerical Study of an Interacting Particle System End of Year 1 Report Tom Hodgson – Supervised by Michela Ottobre &amp; Kevin Painter 2020-05-26 Preface This report is available in two versions: html and pdf. The pdf version is a traditional static report compiled in LaTeX whereas the html version allows for the addition of animations describing the motion of the particles in the systems. Generally, these allow for a much more intuitive view of whats happening, rather than relying on one-dimensional plots. If you’re on the html version and would like a pdf version, simply click on the Adobe Reader logo in the toolbar at the top of this page (or indeed on any page). For the official version please email me. It was created using Bookdown [1]. References "],
["intro.html", "1 Introduction", " 1 Introduction The physical world is filled with systems of interacting agents, from the nanoscale of atomic interactions to the terascale of galaxies filled with stars all exerting a force on one another. Such systems are not limited to physics; biology also provides many examples of complex emergent behaviour such as the slime mould, Dictyostelium. These amoebae coalesce to form slugs consisting of thousands of individuals to move towards an ideal site for the formation of a “fruiting body” as part of their life cycle [2]. Beyond the microsopic scale, “safety in numbers” rules the seas and the skies, with starlings flocking to avoid predation, bees swarming to find new nest sites and fish shoaling to spawn [3]. Even human behaviour can be seen through this lens: traffic jams, opinions and segregation can all be thought of as interacting particle systems [4, 5]. Although in the physical world the laws governing these systems are generally well understood, the same cannot be said of the biological world where common events like animal migration are still yet to be fully explained. By viewing these phenomena as interacting particle systems, we aim to model these interactions and recreate the emergent behaviours in silico. This, in combination with an analytical approach, will lead to better understanding of these complex events. Many attempts have been made to explain the mechanics of these systems, and they broadly fall in to two categories: kinetic models and particle models. Particle models, also known as agent-based models, are favoured by many as they are simple to create and simulate. One can easily develop a set of rules to be followed by individuals and then simulate the behaviour of a group. This was done to great effect by Ballerini et al. who were able to recreate the behaviour of a flock of starlings using very few rules [6]. Kinetic models on the other hand, are obtained in the limit as the number of particles tends to infinity and they describe the evolution of the particles’ density rather than the behaviour of each single agent in the system. It is this aggregation across particles that has seen this method be of great use in statistical mechanics. However, when the interaction between particles becomes more complex, so too does the corresponding PDE which often becomes nonlocal and nonlinear. The methods are thus inextricably linked, however the behaviour of the two can be markedly different, as we will show in this report. We will begin to look at the differences that arise in these two approaches by focussing on one such model: that of Buttà et al. [7]. We are particularly interested in the longtime dynamics of this model. The report is structured as follows: In Section 2 we introduce the main models of our study and present some analytic results. We also set out the main aims of our work and this report. In Section 3, we numerically simulate the models introduced to demonstrate their behaviour and aim to answer questions raised by the analysis. In Section 4, we briefly discuss some related models from the literature and suggest directions for our future research. References "],
["litreview.html", "2 The Model", " 2 The Model The model we study in this report has been introduced in [7]. Let us introduce the model and highlight its main features. Consider a system of \\(N\\) interacting particles evolving on the one-dimensional torus. For each \\(i\\in \\{1, \\dots, N\\}\\), denote by \\(x_t^{i,N} \\in \\mathbb{T}\\) and \\(v_t^{i,N} \\in \\mathbb{R}\\), respectively, the position and velocity of the \\(i\\)-th particle at time \\(t\\); the dynamics of \\(x_t^{i,N}\\) and \\(v_t^{i,N}\\) is described by the following system of SDEs. \\[\\begin{align} \\mathrm{d} {x}_t^{i,N} &amp; = v_t^{i,N} \\, \\mathrm{d} t,\\tag{2.1} \\\\ \\mathrm{d} {v}_t^{i,N} &amp; = - v_t^{i,N} \\mathrm{d} t + G\\left(\\frac{ \\sum_{j=1}^N \\varphi (x_t^{i,N} - x_t^{j,N}) v_t^{j,N}}{ \\sum_{j=1}^N \\varphi (x_t^{i,N} - x_t^{j,N})}\\right) \\mathrm{d} t + \\sqrt{2\\sigma} \\mathrm{d} W_t^{i},\\tag{2.2} \\end{align}\\] where the \\(W^i\\)’s are independent one-dimensional standard Wiener processes and \\(\\sigma&gt;0\\) is fixed. The first equation simply states that the change in position is given by the velocity multiplied by the change in time. The equation governing the velocity however, contains more complexities. The interaction function \\(\\varphi: \\mathbb{T} \\to \\mathbb{R}\\) controls which other particles the i\\(^{\\text{th}}\\) particle interacts with and how strongly. In [7], to avoid technical problems with the denominator of the argument of \\(G\\), the function\\(\\varphi\\) is assumed to be bounded below. We further assume \\(\\varphi\\) is symmetric so that whether a particle is “in front” or “behind” of another has no effect on its interaction strength. More formally, \\[\\begin{equation} \\varphi(x) \\geq \\varepsilon &gt; 0,\\quad \\varphi(x) = -\\varphi(x). \\tag{2.3} \\end{equation}\\] When numerically simulating the system, we are able to drop the positivity assumption. If \\(\\varphi \\equiv 1\\), this corresponds to a mean-field interaction and all particles continually interact with all others equally, irrespective of their relative positions. This type of interaction has been extensively studied and indeed forms the basis for much of statistical physics. This type of “all to all” interaction is not necessarily meaningful for many systems in biology or in the social sciences—for example, people with very different opinions are unliekly to interact. More realistic is that the agent’s ability to interact varies with distance and direction, for example when birds move out of each other’s line of sight. A simple example could be an indicator function on some set \\([-\\gamma, \\gamma]\\). Any particles that lie more than a distance \\(\\gamma\\) apart will no longer interact. For this reason, \\(\\gamma\\) is often called the radius of interaction. This type of interaction function is common in opinion dynamics and leads to the formation of the 2R conjecture [8]. It could be argued that this form of interaction with an instant decline is implausible for many multi-agent systems and more relevant is a slow decay to no interaction. This prompts the introduction of smoothed interaction functions such as bump functions centred at zero. Figure 2.1 illustrates these standard choices. Figure 2.1: Some typical choices of interaction functions for this and other particle models. The solid blue line a) shows a uniform interaction \\(\\varphi(x) \\equiv 1\\), dashed orange b) shows a hard cutoff indicator \\(\\varphi(x) = \\mathbb{I}_{[-\\pi\\gamma,\\pi\\gamma]}(x), \\gamma = 0.2\\) and the dotted green line c) shows a smooth bump function interaction. We thus see that the argument of the function \\(G\\) is a weighted local average of the particles’ velocities. Indeed if \\(\\varphi\\equiv 1\\), this is simply the mean velocity of the ensemble. The perhaps more interesting case is when the interaction is not uniform and there is some spatial heterogeneity. Now we better understand the role of the argument of \\(G\\), what of \\(G\\) itself? The herding function, \\(G:\\mathbb{R}\\to \\mathbb{R}\\), defines how an individual should respond to the weighted average velocity. That is, how does the individual react to the velocities of the particles surrounding it? First notice that when \\(G\\equiv0\\), there is no interaction between particles and the system reduces to \\(N\\) uncoupled Ornstein-Uhlenbeck processes. If there is to be an interaction, it is natural that this function should at some point satisfy \\(G(u)=u\\), so that particles are less perturbed when they are moving with the crowd, and also that \\(G\\) is constructed so that an individual’s velocity moves towards that of the herd. This can be achieved under the following assumption: \\[ \\tag{2.4} G(u) = - G(-u)\\,, \\qquad \\begin{cases} G(u) &gt; u &amp; \\textrm{ if } 0\\leq u&lt;1\\,, \\\\ G(u) &lt; u &amp; \\textrm{ if } u&gt;1\\,. \\end{cases} \\] The choice of \\(G(1)=1\\) here is chosen arbitrarily, the model displays similar dynamics for any \\(\\xi\\) such that \\(G(\\xi)=\\xi\\). We also restrict our attention to the case when there are only two solutions to this equation. This function thus herds the velocities of the particles towards a fixed point. Note also that \\(G\\) controls how quickly an individual moves towards the herd average, or the speed of reaction. Within the modelling literature, it is common to take a step function for \\(G\\): \\[ G(u) = \\frac{u+\\beta \\mathrm{sgn}(u)}{1+\\beta}\\, , \\beta&gt;0 \\] The parameter \\(\\beta\\) controls the strength of the herding function, a typical value is \\(\\beta=2\\). The advantage of this choice is the constant gradient for any \\(u\\neq0\\), but the discontinuity at this point renders them less amenable to analysis—and potentially to numerics, as we will illustrate. For this reason, analysts prefer a smoothed version such as \\[ G_{\\alpha}(u) = \\frac{\\mathrm{atan}(\\alpha u)}{\\mathrm{atan}(\\alpha)}\\, , \\alpha &gt; 0 . \\] A herding function such as this still satisfies the assumptions, whilst also being smooth. The parameter \\(\\alpha\\) again controls the gradient, a typical value is \\(\\alpha = 1\\). Figure 2.2 provides some illustrations and in Section 3 we will consider both of these types of herding functions. Figure 2.2: Some typical choices of herding functions for this model. The solid blue line a) shows the step herding function, \\(G(u) = \\frac{u+\\beta \\mathrm{sgn}(u)}{1+\\beta}, \\beta=2\\). The orange dashed line b) shows the smoothed herding function $G(u) = , =1 $ and the dotted green line shows \\(G(u)=\\frac{h+1}{5}u-\\frac{h}{125}u^3, h=5\\), the herding function used in the numerical simulations of [9]. The dotted grey line is \\(G(u)=u\\). Note the intersection between the grey line and others describes the value to which the average velocity will be herded. For a) and b), this is \\(0, -1, +1\\). For c), the values are \\(0, \\frac{\\pm5}{\\sqrt{3}}\\). Finally, we are left with two terms to discuss. The first term in (2.2) is a friction term, meaning that if there is no noise any individual particle will come to rest in the absence of neighbours and noise. If there are two particles in the system, there are two possibilities: either they do not interact sufficiently and both come to rest; or they coalesce and move together with common velocity. The latter term describes the noise in the system with \\(\\sigma\\) representing the strength. A common characteristic of these types of models is a phase transition between order and disorder dependent on \\(\\sigma\\). Interestingly, it was proved in [7] that this model displays unconditional flocking—there is no transition between ordered motion and disorder. Note that this noise is only present in the velocity equation (2.2), the position equation (2.1) is entirely deterministic. This means the system is not elliptic, and renders it immune to many analysis techniques. When considering a particle model, one often looks to the empirical measure. This can be thought of as a histogram of the positions and velocities at each time \\(t\\). It is defined as \\[ S^N_t(\\mathrm{d} x, \\mathrm{d} v) := \\frac{1}{N}\\sum_{i=1}^N \\delta_{(x^{i,N}_t,v^{i,N}_t)}(\\mathrm{d}x, \\mathrm{d}v). \\] Here \\(\\delta\\) denotes a Dirac measure, that is on any measurable subset \\(A\\) of some set \\(X\\), \\[ \\delta_x(A) = \\begin{cases} 0 \\text{ if } x \\not\\in A,\\\\ 1 \\text{ if } x \\in A. \\end{cases} \\] In [7] it was shown that, under certain assumptions, \\(\\lim_{N\\to \\infty} S^N_t = f_t\\) where \\(f_t\\) solves \\[\\begin{equation} \\tag{2.5} \\partial_t f_t(x,v)=- v\\,\\partial_x f_t(x,v) - \\partial_v\\big\\{\\big[G(M_{f_t}(x)) - v\\big] f_t(x,v) \\big\\} + \\sigma\\,\\partial_{vv}f_t(x,v)\\,, \\end{equation}\\] where \\[\\begin{equation} \\tag{2.6} M_f(x) := \\frac{\\int_{\\mathbb{T}}\\!\\mathrm{d} y \\int_{\\mathbb{R}}\\!\\mathrm{d} w\\, f(y,w)\\,\\varphi(x-y)\\, w}{\\int_{\\mathbb{T}}\\!\\mathrm{d} y \\int_{\\mathbb{R}}\\!\\mathrm{d} w\\, f(y,w)\\,\\varphi(x-y)}\\,. \\end{equation}\\] This is a nonlinear kinetic equation, the analysis of which will help inform our approach to the particle system. We will now summarise what is known about this PDE. The well-posedness of (2.5) was proved in [7] and that a unique solution exists in a weighted \\(L^1\\) space \\[ L^1\\left(\\sqrt{1+v^2}\\mathrm{d}x \\mathrm{d}v\\right) := \\bigg\\lbrace f:\\mathbb{T}\\times\\mathbb{R} \\to \\mathbb{R} : \\int_{\\mathbb{T}}\\int_{\\mathbb{R}} |f| \\sqrt{1+v^2} \\mathrm{d}v\\mathrm{d}x&lt; \\infty \\bigg\\rbrace. \\] A complete analytic description of the long-time behaviour of (2.5) seems to be out of reach (we explain below why this is the case); however, the asymptotic behaviour of this dynamics has been characterized in some simplified cases, namely 1) when the density \\(f_t\\) is space-homogeneous, i.e. when it does not depend on the space-variable \\(x\\); 2) when the function \\(\\varphi\\) is a perturbation of the mean field case and 3) when the non- linearity (2.6) is simplified. Let us comment on these three cases in turn. If we consider only space- homogeneous densities, i.e. solutions of the form \\(f_t(v)\\), then the evolution (2.5)-(2.6) reduces to the (much) simpler dynamics \\[\\begin{equation} \\tag{2.7} \\partial_t f_t(v)=- \\partial_v\\big\\{\\big[G(M(t)) - v\\big] f_t(v) \\big\\} + \\sigma\\,\\partial_{vv}f_t(v)\\,, \\end{equation}\\] where \\(M(t):= \\int_{\\mathbb{R}}v\\, f_t(v) \\mathrm{d}v\\) solves the one-dimensional ODE \\[\\begin{equation} \\tag{2.8} \\dot M (t)= G(M(t))-M(t) . \\end{equation}\\] In other words, the nonlinearity \\(M_{f_t}(x)\\) reduces to being the average velocity \\(M(t)\\); because one can close an equation on \\(M(t)\\), one can now regard \\(M(t)\\) as being a time-dependent coefficient; hence, in this space-homogeneous case, the nonlinear dynamics (2.5) reduces to a linear, non-autonomous PDE. In this case one can easily prove that there exist three stationary solutions of (2.7), namely three Gaussians \\[\\begin{equation} \\tag{2.9} \\mu_{\\pm}= \\frac{1}{Z_{\\pm}} e^{-\\frac{(v\\mp 1)^2}{2 \\sigma}} \\quad \\mbox{ and } \\quad \\mu_{0}= \\frac{1}{Z_{0}} e^{-\\frac{v^2}{2 \\sigma}} \\, , \\end{equation}\\] where \\(Z_{\\pm}\\) and \\(Z_0\\) are appropriate normalization constants. Note that these are the only three stationary solutions of the dynamics (2.7)-(2.8), irrespective of the value of \\(\\sigma\\). This is the unconditional flocking mentioned earlier. Moreover, if the initial profile \\(f_0\\) has positive mean, i.e. if \\(M(0)&gt;0\\) (\\(M(0)&lt;0\\), respectively) then the dynamics converges exponentially fast to the stationary measure with positive mean, \\(\\mu_+\\) (\\(\\mu_-\\), respectively). See Appendix A for an explanation. As we have mentioned, the study of the long-time behaviour of the full model (2.5)-(2.6) is quite challenging. In particular one can easily see that the three measures (2.9) are still stationary solutions of (2.5)-(2.6); however the authors of [7, 9] were not able to prove that these are indeed the only three stationary measures; this is a conjecture that we aim to confirm numerically. The fact that the measures (2.9) are the only stationary solutions of (2.5)-(2.6) has been proven analytically only in the case in which the function \\(\\varphi\\) is chosen to be a small oscillation around the constant function \\(\\varphi \\equiv 1\\); to be more precise, this has been proven (under some assumptions on the regularity of the solution), only in the case in which \\[ \\varphi(x)= 1+ \\lambda \\psi(x), \\quad \\mbox{where } 0&lt;\\lambda \\ll 1, \\int_{\\mathbb{T}} \\psi(y) \\, dy =0 \\,. \\] The difficulty in solving the stationary problem associated with (2.5)-(2.6) comes from the fact that such a model is not in gradient form. In [9], the authors introduced a model similar to (2.5)-(2.6), namely they study the following non-linear PDE \\[\\begin{equation} \\tag{2.10} \\partial_t f_t(x,v)=- v\\,\\partial_x f_t(x,v) - \\partial_v\\big\\{\\big[G(\\tilde{M}_{f_t}(x)) - v\\big] f_t(x,v) \\big\\} + \\sigma\\,\\partial_{vv}f_t(x,v)\\,, \\end{equation}\\] where this time nonlinearity \\(\\tilde{M}_f\\) is given by \\[\\begin{equation} \\tag{2.11} \\tilde{M}_f(x) := {\\int_{\\mathbb{T}}\\!\\mathrm{d} y \\int_{\\mathbb{R}}\\!\\mathrm{d} w\\, f(y,w)\\,\\varphi(x-y)\\, w}\\,. \\end{equation}\\] This corresponds to the following particle system \\[\\begin{align} \\mathrm{d} {x}_t^{i,N} &amp; = v_t^{i,N} \\, \\mathrm{d} t, \\tag{2.12} \\\\ \\mathrm{d} {v}_t^{i,N} &amp; = - v_t^{i,N} \\mathrm{d} t + G\\left(\\frac{ \\sum_{j=1}^N \\varphi (x_t^{i,N} - x_t^{j,N}) v_t^{j,N}}{N}\\right) \\mathrm{d} t + \\sqrt{2\\sigma} \\mathrm{d} W_t^{i}.\\tag{2.13} \\end{align}\\] Comparing this with (2.1)-(2.2), notice that the only difference is in the denominator of the argument of the herding function \\(G\\). This choice of normalisation is akin to scaling the interaction by the total number of particles in the system. For this reason, we call this global scaling. In contrast, we call the interaction term in (2.1)-(2.2) locally scaled as there the interaction is normalised by the amount of particles that are interacting with an individual. Scaling globally may seem counterintuitive: the dynamics of a flock of birds in one continent are surely not affected by a flock in another continent. We will show that in some regimes (for example with a large particle count \\(N\\) and large support of \\(\\varphi\\)) these models are clearly very similar. We only expect a difference in the dynamics for smaller particle numbers, or where there is a large distance between agents. Global scaling is a common modelling assumption in the analysis literature as it reduces the convergence of the measure to a well-studied case—that of the mean field interaction. In Section 3 we will investigate numerically the difference in these two models. For the globally scaled model (2.10), it has been shown that the only stationary states are (2.9) [9]. Less is known about the particle model (2.1)-(2.2). When noise is present in the system, the stationary states of (2.5) correspond to metastable states in the particle system. It can be shown that when \\(\\varphi\\equiv1\\) this system has an invariant measure with mean zero, see Appendix B. We conjecture that the same is true when \\(\\varphi \\not\\equiv 1\\), and aim to prove this in a future work. If there is no noise in the system and a uniform interaction is assumed, then the velocity of all particles converges to the point \\(\\xi\\), where \\(\\xi\\) is one of the points such that \\(G(\\xi)=\\xi\\). In summary, the two models presented in [7] &amp; [9] are very similar, however this discrepancy in the argument of the herding function will in fact have large effects on the dynamics. Furthermore, the analytic results for the locally scaled particle system (2.1)-(2.2) and its corresponding kinetic PDE (2.5) do not give a complete picture of the dynamics. We thus turn to a numerical approach to inform the analysis in the case when the interaction is not uniform. Our first question is whether a change in \\(\\varphi\\) affects the rate of convergence to stationarity. Second, are there any spatially inhomogeneous solutions to the kinetic equation? Do particle distributions always tend towards being uniform in space? In addition, when \\(\\varphi\\not \\equiv1\\), the locally scaled (2.1)-(2.2) and globally scaled (2.12)-(2.13) models are no longer equivalent. What differences in behaviour do they exhibit? References "],
["num-studies.html", "3 Numerical Studies 3.1 On the Difference Between Local and Global Scaling 3.2 Simulation of Kinetic Models", " 3 Numerical Studies Models of particle systems contain complex dynamics and static plots often do not best illustrate their behaviour. For this reason, animations of the systems have also been produced to accompany this report. They are available at https://tom271.github.io/InteractingParticleSystems/ This is a precursor to a webpage that will accompany a future work in which we hope to produce interactive simulations of the system through a Dash app1. Where animations are available, they will be clearly signposted in figure captions. For more details on the implementation, see Appendix D. We begin this section by confirming known results numerically, before starting to answer some of the questions raised in the previous section. To demonstrate some possible behaviours of the locally scaled particle system (2.1)-(2.2), we simulate it using a smooth herding function(\\(G_1\\)) with \\(\\varphi\\equiv1\\) and \\(\\sigma=1\\) for 1000s. The aim is to corroborate the analytic results for this system. Figure 3.1 shows a histogram of the particles’ positions at \\(t=0\\) and \\(t=1000\\). We see that they appear to converge to either \\(\\mu_+\\) or \\(\\mu_-\\), the known stationary measures of the (2.5), depending on the sign of the average velocity of the initial data. This is in agreement with the results of the previous section (and Appendix A). However, if the simulation is ran for longer, we see that these are only metastable states and instead the average velocity of the particles switches between \\(+1\\) and \\(-1\\). A similar effect can be produced by increasing the noise or reducing the number of particles. Assuming uniform interaction, we have proved that the stationary measure of the particle system has mean zero. This is numerical corroboration of that fact. This switching of stability is a well-known phenomenon and indeed is behind the strength of many optimisation algorithms such as SGLD [10]. Figure better illustrates this switching by reducing the number of simulated particles. By removing the noise present in (2.1)-(2.2), we can remove this effect. If we assume space homogeneity, set \\(\\varphi \\equiv 1\\) and remove the noise the system (\\(\\sigma=0\\)) reduces to \\[\\begin{align} \\mathrm{d} {v}_t^{i,N} &amp; = - v_t^{i,N} \\mathrm{d} t + G\\left(M(t)\\right) \\mathrm{d} t , \\tag{3.1} \\end{align}\\] where \\[ M(t) = \\frac{1}{N}\\sum_{j=1}^N v^{j,N}_t \\] Summing over \\(i\\) and dividing by \\(N\\) both sides of (3.1) one obtains a closed equation for the mean velocity, that is \\[\\mathrm{d}M(t) = \\left(G(M(t)) - M(t)\\right)\\mathrm{d}t. \\] This is in accord with the result for the space homogeneous PDE (2.7) when \\(\\varphi\\equiv1\\) and has stationary points where \\(M(t) = G(M(t))\\). For our examples, this corresponds to \\(M(t) =-1, 0 ,+1\\). Figure 3.1: Regardless of the initial data, the system always converges to \\(\\mu_+\\) or \\(\\mu_-\\) when \\(\\varphi\\equiv1\\). Here, initial configurations with positive initial average velocity are coloured green and those with negative initial velocity are orange. Here we simulate \\(1000\\) particles under a uniform interaction with a smooth herding function and \\(\\sigma=1\\). After \\(1000s\\) a histogram is used as an approximation for the empirical measure and plotted. The initial data are randomly selected Gaussians with means varying between \\(-5\\) and \\(5\\) while the standard deviations lie between \\(0.5\\) and \\(3\\). Here a smooth herding function \\(G_1\\) was used, and a timestep of \\(\\Delta t =0.01\\) in an Euler-Maruyama scheme (see Appendix D) Figure 3.2: Evolving the dynamics for a longer time suggests an invariant measure with mean \\(0\\) due to the possibility of jumping between states. Here \\(N=50 , \\sigma=1\\) and a smooth herding function is used. The grey dashed lines show Gaussian distributions centred at \\(\\pm1\\) for reference. In the absence of noise, we can time the convergence to equilibrium for different initial data as we know that no switching is possible. Once a baseline has been set for uniform interaction, we can compare this with non-uniform interactions, \\(\\varphi\\not\\equiv1\\). To do this we use the following initial setup. We start the particles in two diametrically opposite clusters of equal width: one containing one third of the total particles with positive velocity, and the other containing two thirds of the particles with negative velocity. This allows for an interesting setup—most particles begin movement with negative velocity yet the average velocity can be positive. We give each particle in the small cluster velocity \\(1.8\\), while in the larger cluster all particles have velocity \\(-0.2\\). The initial average velocity of the system is thus \\(\\frac{7}{15}\\approx 0.467\\). \\[\\begin{align*} x^{i,3N}_0 &amp;\\sim U\\left[\\pi-\\frac{\\pi}{10},\\pi+\\frac{\\pi}{10}\\right], &amp;&amp;v^{i,3N}_0 = -0.2 \\qquad \\text{ for } 1\\leq i\\leq 2N,\\\\ x^{i,3N}_0 &amp;\\sim U\\left[-\\frac{\\pi}{10},\\frac{\\pi}{10}\\right], &amp;&amp;v^{i,3N}_0 = 1.8\\qquad \\text{ for } 2N+1\\leq i\\leq 3N. \\end{align*}\\] With this initial distribution, the system is very far from the stationary distribution of the PDE. To control the influence of the interaction function, we use the hard cutoff indicator function described in Figure . By varying the value of \\(\\gamma\\), the range of interaction can be controlled. Note that this interaction is not uniformly bounded above zero, and as such does not satisfy the assumptions (2.3). For the herding function we use the step function common in the modelling literature. \\[\\begin{align*} \\varphi(x) &amp;= \\mathbb{I}_{[-\\pi\\gamma,\\pi\\gamma]}(x)\\,,\\gamma\\in \\bigg\\lbrack-\\frac{1}{2},\\frac{1}{2}\\bigg\\rbrack,\\\\ G(u) &amp;= \\frac{u+\\beta \\mathrm{sgn}(u)}{1+\\beta}\\, , \\beta&gt;0 \\end{align*}\\] As a guide, doubling the radius of interaction \\(\\gamma\\), gives the fraction of the torus which can be seen by each particle, and \\(\\gamma= \\frac{1}{2}\\) is equivalent to \\(\\varphi\\equiv1\\). Figure shows how this affects the speed of convergence. Here the number of particles \\(N\\) has no discernible effect on the dynamics. Surprisingly, we see here that the average velocity can settle into a periodic orbit not centred at \\(-1,0\\) or \\(1\\), as we might na\"ively expect. The system does not converge to \\(\\pm1\\) either in law or in the sense of ergodic averages—for an explanation of the difference, see Appendix C. This cannot occur in the noisy particle system and indeed is unexpected. However to see how this can occur, consider a system containing just three particles. There are three possible behaviours From the two particles case, we know the particles will either not interact and come to rest or they can coalesce and move in unison. Both of these are possible in a three particle system however a third option appears. Two of the particles can coalesce and quickly converge to velocity \\(\\pm1\\). When this occurs, decay to rest is no longer possible. Upon meeting the third particle, it can either be absorbed into the cluster or the interaction can be insufficient and it gets left behind. The cluster containing only two particles will then accelerate back to its preferred velocity while the third particle returns to rest—until the cluster comes back around. This is what produces these at first counterintuitive periodic patterns. The same is happening here for very weak interactions. See online for the three particle case and the one present in Figure 3.3. Figure 3.3: In the deterministic system (\\(\\sigma=0\\)), unexpected behaviours can occur. For \\(\\gamma=0.05\\), we see the average velocity settle into a periodic orbit, and for \\(\\gamma=0.1\\), a convergence to \\(-1\\). In a), we see the time taken for the average velocity to converge. Although skewed by the periodic trajectory, it is possible to see the dependence on \\(\\gamma\\). In b), the colour refers to the size of \\(\\gamma\\). In all these simulations, we use a step herding function and the opposing clusters initial setup. Also surprising here is that the initial average velocity of the ensemble, \\(M(0)\\), does not govern the convergence as it did for both the PDE and the particle system when \\(\\varphi \\equiv 1\\). We see that there is a radius \\(\\gamma = 0.05\\) for which the system converges to velocity \\(-1\\)—at odds with its initial average velocity, \\(M(0)=\\frac{7}{15}\\). This occurs as the two initial clusters do not interact quick enough. An intuitive reasoning for this is that, seen separately, each cluster is converging to a different value. The cluster at \\(0\\) has positive velocity and thus accelerates towards average velocity \\(1\\), while the cluster at \\(\\pi\\) has negative velocity and accelerates towards \\(-1\\). This can cause the average velocity of the entire ensemble to become negative before sufficient interaction occurs. Thus by the time the interactions begin, the system as whole will converge to \\(-1\\). We see this happening whenever \\(\\gamma \\leq 0.1\\). Now that the role of the interaction function is better understood, we take a similar approach in our study of the herding function \\(G\\). In particular, we are interested in how the presence of a discontinuity affects the dynamics. As such, we take the smoothed herding function \\[ G_{\\alpha}(u) = \\frac{\\mathrm{atan}(\\alpha u)}{\\mathrm{atan}(\\alpha)}\\, , \\alpha &gt; 0 . \\] The parameter \\(\\alpha\\) allows us to control the gradient of the herding function. As $, G(u) $ tends towards a step function. Figure 3.4 shows the effect of three different values: \\(\\alpha = 1, 5,10\\). It can be seen that as the herding function gets steeper, periodic orbits become more prevalent. The system behaves unlike we expect from any of the analytic results. Figure 3.4: As the steepness of the herding function increases, we see more and more erratic behaviour for small \\(\\gamma\\). Here the herding function is smooth, \\(G_{\\alpha}\\) with \\(\\alpha=1,5,10\\). The interaction function has a hard cutoff. To aid in the stochastic case, it will be pertinent to examine the existence of spatially inhomogeneous distributions in the deterministic case. Starting with the two clusters of uneven density described above, we evolve the dynamics for \\(50s\\) seconds before counting the number of clusters that remain. We do this by looking at the histogram at the final time. The ``eyeball metric’’ was chosen after considering other methods such as \\(k\\)-means, Jenks optimisation or counting the minima of a kernel density estimate for its ease of adaptation to the torus. Table 3.1 below shows the number of clusters present after \\(50s\\). We see that as \\(\\gamma\\) increases, the number of clusters present decreases. Note that despite the clusters, this system is still converging to a uniform distribution in the sense of ergodic averages. Table 3.1: N=24 N=168 N=360 N=456 \\(\\gamma=0.05\\) 3 2 3 2 \\(\\gamma=0.15\\) 2 2 2 1 \\(\\gamma=0.3\\) 1 1 1 1 \\(\\gamma=0.45\\) 1 1 1 1 In the deterministic setting, we have seen that a larger interaction radius speeds up convergence and if the radius is too small it is possible to converge to a different value than that expected. We also see that stable clusters form—as expected for a deterministic system. Surprisingly, we saw periodic average velocities emerge in many cases when the interaction radius was small. The herding function also affects convergence, with the presence of a steep gradient also causing periodic average velocities. Now, we turn our attention back to the stochastic system to see whether these phenomena persist in the presence of noise. We immediately expect to lose these periodic average velocities, as well as any clusters present. To see the influence of noise, Figure 3.5 shows the effect on average velocity when \\(\\sigma=0.1\\). Again we begin the evolution from two diametrically opposed clusters initially travelling with deterministic speeds \\(-0.2\\) and \\(1.8\\). It can be seen that no periodic orbits persist and all systems converge to \\(\\pm1\\) except when \\(\\gamma=0\\). Furthermore, smaller values of \\(\\gamma\\) cause convergence to slow down, similar to the deterministic case. The difference here is clearer as there are no periodic orbits skewing the dataset. Figure 3.5: One realisation of the locally scaled particle model with the step herding function; hard cutoff interaction function with \\(\\gamma = 0, 0.05,0.1,\\dots,0.5\\) and \\(N=24, 72, 120, \\dots, 408\\). The initial distribution is two opposing clusters travelling in opposite directions. This is the same setup as Figure 3.3 but with \\(\\sigma=0.01\\). We see that there no periodic orbits persist, unlike the deterministic case. The colour of the lines corresponds to the interaction radius \\(\\gamma\\). To see if the volatile behaviour produced by increasing the gradient of \\(G\\) persists in the presence of noise, we now repeat the earlier experiment but here with \\(\\sigma=0.01\\). Figure 3.6 shows the result: when there is noise in the system, its effect is much reduced. Note however that for \\(\\gamma = 0.1\\), the system still settles into an unexpected state before the noise pushes the average velocity towards \\(-1\\). Figure 3.6: As in Figure 3.4, here the herding function is smooth, \\(G_{lpha}\\) with \\(lpha=1,5,10\\) however here we introduce some noise, \\(\\sigma=0.01\\). The interaction function has a hard cutoff across a range of \\(\\gamma\\). Here we see that the noise removes the erratic periodic behaviours regardless of gradient. However note that there is an effect for some \\(\\gamma\\), particularly when \\(\\alpha=10\\). When there is noise in the system, we can compare the rate at which clusters break to the deterministic system where it is possible for clusters to persist. To do this, we calculate the discrete \\(\\ell^1\\) distance between the uniform distribution and a histogram of all particle positions at time \\(t\\). This will give a quantitative measure of how quickly clusters disperse in the stochastic regime. Figure shows the difference in \\(\\ell^1\\) distance for a \\(N=87\\) and $= 0.02,0.04,0.09 $. It can be seen that regardless of \\(\\gamma\\), the noise in the velocity variable quickly breaks any clusters present. Note that the \\(\\ell^1\\) distance does not converge to 0 as expected as there is not enough particles in the system for the histogram to ever be a good approximation to the uniform distribution at any given time point. These results suggest that spatially inhomogeneous invariant distributions do not exist for the particle system even when \\(\\varphi\\not \\equiv 1\\). Figure 3.7: A comparison of \\(\\ell^1\\) distance between a histogram of particle positions and the uniform distribution. The deterministic system (orange) retains clusters and therefore a high distance, while even a small amount of noise (here, \\(\\sigma=0.05\\)) disperses the particle positions. 3.1 On the Difference Between Local and Global Scaling If the system is scaled globally, as in (2.12)-(2.13), we see a difference in speed of convergence. Figure 3.8 shows that for smaller values of \\(\\gamma\\), the system has a much larger convergence time. The systems appear to settle into a steady state dependent on the value of \\(\\gamma\\), before moving out of this state and towards \\(-1\\). Many trajectories settle into a periodic pattern very close to \\(-1\\)—these are caused by one or two particles remaining out of a large cluster as discussed in the deterministic case.As \\(\\gamma\\) gets larger, the local scaling converges to the global scaling and we see less difference in the trajectories (cf. Figure 3.3. This is thought to be because for this interaction function, the interaction is always less than 1. Hence, the strength of the interaction in the local scaling will always be greater than that in the global scaling, due to the monotonicity of \\(G\\). Figure 3.8: If the model is scaled globally, low values of \\(\\gamma\\) have a much larger convergence time. Here the system was evolved for \\(500s\\) from an initial configuration of two clusters of uneven density. For higher interaction radii, behaviour is very similar. Some systems persist in a periodic trajectory for even larger times (tested up to \\(1000s\\)). Note the logarithmic scale. 3.2 Simulation of Kinetic Models Simulating the kinetic models is an ongoing work. Many standard numerical techniques seem unsuitable for this model. The PDE (2.5) is degenerate, in the sense that the noise is not elliptic. By only acting on the velocity, we must be very careful not to introduce any artificial diffusion into the space variable that is a common pitfall of many standard techniques such as upwind schemes (Figure 3.9 illustrates this for the one-way wave equation). Doing so would surely break any clusters that may form under the dynamics. As mass is also conserved in this system (See Appendix A), we also must be careful not to use any techniques that cause mass loss or allow negative solutions. To combat this, we have begun adapting 2DChebClass, a pseudospectral method originally developed for the solution of problems arising in dynamical density functional theory (DDFT) [11, 12]. These provide exponential accuracy in the number of grid points, meaning the mesh can be much coarser than in a typical finite volume scheme. The drawback of these methods is that they are limited in the geometry of the domain in which they can solve problems. As here we are only considering a two dimensional model on \\(\\mathbb{T}\\times\\mathbb{R}\\), this is not an issue. Figure 3.9: Even for smooth initial data, a first order upwind scheme exhibits severe artificial diffusion. Both plots show the one way wave equation \\(u_t-u_x=0\\) with an indicator initial profile (a) and a Gaussian profile (b). Figure 3.10: Simulating the kinetic model (2.5) using 2DChebClass. Here we use a smooth herding function, uniform interaction (\\(\\varphi=1\\)) and initial data \\(f_0 = \\exp(-(v-1)^2/2)(\\exp(\\sin(x))+2)\\). The equation is solved on \\(\\mathbb{T}\\times[-5,5]\\), with 10 mesh points in both dimensions. The solution appears to contradict the non-negativity, however this is just an artefact of the interpolating method used for plotting—the solution calculated is always non-negative. References "],
["future.html", "4 Future Work 4.1 Other Related Literature", " 4 Future Work Throughout this report we have suggested areas for future work. In this section we summarise those and expand on our prospective approach, as well as discuss some related models in the literature. 4.1 Other Related Literature One of the classic studies into collective motion was the work of Vicsek et al. [13] in which what is now known as the Vicsek model was proposed. This is a particle model which exhibits phase transitions despite having a very simple setup. In this model, the particles align themselves based on the average velocities of all the other particles within distance \\(R\\). Every particle has the same speed, and at each timestep the direction, \\(\\theta\\) is update according to the following rule: \\[\\theta (t + \\Delta t) = \\langle \\theta (t)\\rangle_R + \\Delta \\theta.\\] Here, \\(\\langle \\theta (t)\\rangle_R\\) denotes the average direction of the velocities of all particles within a distance \\(R\\) of the i\\(^{\\text{th}}\\) particle. The term \\(\\Delta \\theta\\) is a random variable uniformly distributed on \\([-\\eta/2,\\eta/2]\\). With only three free parameters for a given system size (noise \\(\\eta\\), system density \\(\\rho\\) and speed of particles), this model seems very simple. However, it was shown that the system exhibits a phase transition between ordered and unordered motion. This model was designed to be simple, and incorporates some unphysical characteristics such as a hard interaction cutoff and constant particle velocity. Other models have also been proposed, such as the Cucker-Smale model which removes these two restrictions [14]. Both these models have focussed on capturing the essence of interacting particles and find the simplest set of rules that would emulate flocking behaviour. This allows for analysis and strict bounds to be given on phase transitions. Others introduced more complex rules to better emulate biological systems. Through studying natural systems, biologists can develop logical rules for interaction. Couzin et al. follow this approach in developing their model [15]. So far we have been focussing on a one-dimensional analogue of the Vicsek model, sometimes called the Czirok model (after [16]), that was recently analysed by Garnier et al.[9] and a more complex extension developed by Butt`a et al. [7]. This reduction to one dimension does not make the analysis any easier, in fact in this model the difficulty lies in the interaction term, as we will see. In the remainder of this section we will summarise the contents of these two papers and present some related theory on the longtime behaviour of particle systems. To be able to study the longtime dynamics of interacting particle systems, we will need some theory. The majority has been developed for the case when the interaction takes a mean-field form, and the equation can be written in gradient form. This is not suitable in our case, as it is not possible to write this system in gradient form. We aim to follow the work of Stuart and Mattingly [17]; and Rey-Bellet [18] to find the invariant measures of the particle system (2.1)-(2.2) in the case when \\(\\varphi \\not \\equiv 1\\). We conjecture that the system in fact possesses a unique invariant measure with mean zero due to the noise term. Beyond the analysis, the next step for us is to develop further 2DChebClass for our needs, and to simulate the PDE (2.5) in the same regimes in which we have simulated the particle system. This will allow us to compare the dynamics directly. More importantly, we wish to provide numerical verification of the conjecture that this model has only three invariant measures. A possible extension of the work here is to investigate the change in dynamics if we do not assume that the particles behave similarly. Again a study could be done on the difference in behaviour in following either a kinetic or particle-based approach. This leads to so-called leader-follower dynamics [19]. To summarise, in this report we have shown numerically that the interaction has an effect on the rate of convergence. If the interaction function has a larger support, the rate of convergence increases. We showed that unexpected periodic average velocities arose in the deterministic system and that this behaviour was removed with the introduction of noise. Adding noise also removed any spatial inhomogeneities seen when \\(\\sigma=0\\). This adds weight to our conjecture that there are no spatially inhomogeneous invariant measures for the particle system. We also demonstrated some of the differences between local and global scaling in the model and look to develop this further. Finally, we discussed the simulation of the kinetic model (2.5) and our plans for future work. References "],
["app-prodmeasure.html", "A Product Measure is a Solution to Kinetic PDE A.1 Product Distribution is a Solution A.2 Convergence Depends on Initial Average Velocity", " A Product Measure is a Solution to Kinetic PDE A.1 Product Distribution is a Solution Consider the stationary problem, \\[\\begin{equation} \\tag{A.1} - v\\partial_x f_t(x,v) + \\partial_v v f_t(x,v)- \\partial_v \\left[ G\\left(M(t,x)\\right)f_t(x,v))\\right] + \\sigma \\partial_{vv} f_t(x,v) = 0. \\end{equation}\\] Here we will show that a product distribution is a solution of the stationary problem (A.1) if and only if it is uniform in the space variable and Gaussian in the velocity variable. Moreover, the Gaussian velocity can only have variance \\(\\sigma\\) and mean \\(u\\) where \\(u\\) solves \\(G(u)=u\\). Let \\(f(x,v) = g(x)h(v)\\), for some functions \\(g,h\\). Note the lack of dependence on \\(t\\) as we seek stationary solutions. Then, \\[\\begin{align*} \\partial_x g(x)h(v) &amp;= h(v)\\partial_xg(x)\\\\ \\partial_v g(x)h(v) &amp;= g(x)\\partial_v h(v)\\\\ \\partial_{vv} g(x)h(v) &amp;= g(x)\\partial_{vv}h(v) \\end{align*}\\] The averaging function \\(M(t,x)\\) can also be simplified. \\[ \\begin{aligned}[t] M(x) &amp;= \\frac{\\int_\\mathbb{T}\\mathrm{d} y\\int_\\mathbb{R}\\mathrm{d} w \\,g(y)h(w)\\varphi(x-y) w}{\\int_\\mathbb{T}\\mathrm{d} y\\int_{\\mathbb{R}} \\mathrm{d} w \\,g(y)h(w)\\varphi(x-y)}\\\\ &amp;= \\frac{\\left( \\int_\\mathbb{R}\\mathrm{d} w \\,w h(w)\\right) \\int_\\mathbb{T}\\mathrm{d} y\\,g(y)\\varphi(x-y)}{\\left( \\int_{\\mathbb{R}}\\mathrm{d} w \\,h(w)\\right) \\int_{\\mathbb{T}}\\mathrm{d} y\\,g(y)\\varphi(x-y)} &amp;&amp; \\text{by independence}\\\\ &amp;= \\int_{\\mathbb{R}} \\mathrm{d} w \\,w h(w) &amp;&amp; \\text{}\\\\ &amp;= \\bar{v} \\end{aligned} \\] Note here that the average velocity depends on the distribution. Substituting this all in to (A.1) gives, \\[\\begin{equation} - vh(v)\\partial_x g(x) + g(x)\\partial_v\\left[(v-G(\\bar{v}))h(v) + \\sigma \\partial_v h(v)\\right] = 0. \\tag{A.2} \\end{equation}\\] Then, integrating in space, \\[\\begin{align*} &amp;\\partial_v\\left[(v-G(\\bar{v}))h(v) + \\sigma \\partial_v h(v)\\right] = 0\\\\ \\implies &amp; \\left[(v-G(\\bar{v}))h(v) + \\sigma \\partial_v h(v)\\right] = \\mathrm{const.} \\end{align*}\\] where the first addend is zero by periodicity of the torus. So, \\[ \\partial_v h - \\left(\\frac{G(\\bar{v})-v}{\\sigma}\\right)h + \\mathrm{const.} = 0. \\] This can be solved using an integrating factor to give \\[ h(v) = C\\mathrm{e}^{\\frac{2G(\\bar{v})v-v^2}{2\\sigma}}. \\] Applying the normalising constraint gives \\[ h(v) = \\frac{1}{\\sqrt{2\\pi\\sigma}}\\mathrm{e}^{\\frac{-(v-G(\\bar{v}))^2}{2\\sigma}}. \\] If we now multiply by \\(v\\) and integrate with respect to velocity we obtain \\[ \\bar{v} = \\int v h(v)\\mathrm{d}v = \\int v \\frac{1}{\\sqrt{2\\pi\\sigma}}\\mathrm{e}^{\\frac{-(v-G(\\bar{v}))^2}{2\\sigma}}\\mathrm{d}v = G(\\bar{v}). \\] So the velocity distribution at stationarity is Gaussian with mean \\(G(\\langle w \\rangle )\\) and variance \\(\\sigma^2\\). If we take the smooth herding function \\(G(u) = \\frac{\\mathrm{atanh}(u)}{\\mathrm{atanh}(1)}\\), this corresponds to a Gaussian with mean \\(-1,0\\) or \\(+1\\). Returning to Equation (A.2), we see that the first addend must be identically zero for the equation to hold. In particular, this implies \\(\\partial_{x} g(x) = 0\\), due to the positivity of \\(h(v)\\) (as it is a Gaussian). We thus deduce that \\(g\\) must be the uniform measure on \\(\\mathbb{T}\\). Hence, \\(f(x,v)=g(x)h(v)\\) is a stationary solution of the PDE (A.1). A.2 Convergence Depends on Initial Average Velocity If we further assume space homogeneity of the PDE, that is, \\(f_t(x,v) = h_t(v)\\), we can infer exactly how the average velocity controls the dynamics. If we multiply the space homogeneous PDE by \\(v\\) and integrate with respect to velocity we obtain \\[ \\int v \\partial_t h_t(v)\\mathrm{d} v = -\\int v \\partial_v G(M(t)) h_t(v)\\mathrm{d} v + \\int v \\partial_v vh_t(v)\\mathrm{d} v + \\sigma \\int v \\partial_vv h_t(v)\\mathrm{d} v \\] Integrating by parts then gives an autonomous first order ODE. \\[ \\partial_t M(t) = G(M(t) ) - M(t) \\] As this equation is autonomous, the average velocity must be monotone. There are stationary points whenever the average velocity solves $G(M(t)) =M(t) $, as expected from the product solution found previously. For example, if we take \\(G(u) = \\frac{\\mathrm{atanh}(u)}{\\mathrm{atanh}(1)}\\), then the average velocity converges to $ -1, 0 $ or \\(+1\\), depending on the sign of the initial average velocity. If \\(M(0) &gt; 0\\), then \\(M(t) \\to +1\\). Similarly, the stationary point at \\(-1\\) is stable and the system will converge there for any configuration such that \\(M(0) &lt; 0\\). Finally, \\(0\\) is an unstable equilibrium, only attainable when \\(M(0) = 0\\). See Figure for a phase plane diagram of the system when \\(G\\) is the inverse hyperbolic tangent. In a similar manner, we can also obtain equations for other moments. Denoting the \\(n^{\\text{th}}\\) moment at time \\(t\\) by \\(M_n(t)\\), we obtain: \\[\\begin{align*} \\dot{M}_0(t) &amp;= 0\\\\ \\dot{M}_1(t) &amp;= G(M_1(t))-M_1(t) &amp;&amp; \\text{as above}\\\\ \\dot{M}_2(t) &amp;= 2\\left[G(M_1)M_1-M_2+\\sigma \\right]. \\end{align*}\\] The first of these equations shows that the system conserves mass, an important property we must preserve when simulating numerically. The second is a restatement of the result above, and gives convergence of the average velocity. The final equation gives the convergence of the variance. "],
["app-PSmeanZero.html", "B Particle System has an Invariant Measure with Mean Zero", " B Particle System has an Invariant Measure with Mean Zero Consider the evolution described by , with \\(\\varphi\\equiv 1\\), that is the space homogeneous particle system with uniform interaction: \\[\\begin{align} \\mathrm{d} {v}_t^{i,N} &amp; = - v_t^{i,N} \\mathrm{d} t + G\\left(\\frac{ \\sum_{j=1}^N v_t^{j,N}}{N}\\right) \\mathrm{d} t + \\sqrt{2\\sigma} \\mathrm{d} W_t^{i}, \\quad i = 1,\\dots N. \\tag{B.1} \\end{align}\\] Notice that this is equivalent to the globally scaled particle model (2.12)-(2.13) when \\(\\varphi\\equiv1\\). We will show that this equation has an invariant measure with mean zero. We believe this to be unique and will prove as such in a future work. Let \\(G\\) be smooth and define \\(\\bar{v}_t = \\sum_{i=1}^N v_t^{i,N}\\). Then, by summing over \\(i\\) in (B.1), \\(\\bar{v}_t\\) solves \\[\\begin{equation} \\tag{B.2} \\mathrm{d}\\bar{v}_t = \\left[-\\bar{v}_t+G(\\bar{v}_t)\\right]\\mathrm{d}t + \\sqrt{2\\sigma}\\mathrm{d}W_t, \\quad \\bar{v}_t \\in \\mathbb{R}. \\end{equation}\\] Let \\(V(x) = \\frac{x^2}{2} + \\tilde{V}(x)\\), where \\(\\tilde{V}(x)\\) is such that \\(G(x) = -\\tilde{V}&#39;(x)\\) so that \\[ -V&#39;(\\bar{v}) = -\\bar{v} + G(\\bar{v}). \\] Equation (B.2) can be rewritten as \\[\\begin{equation} \\tag{B.3} \\mathrm{d}\\bar{v}_t = -V&#39;(\\bar{v})\\mathrm{d}t + \\sqrt{2\\sigma}\\mathrm{d}W_t, \\quad \\bar{v}_t \\in \\mathbb{R}. \\end{equation}\\] Now $({v}) = - _{-}^{{v}} G(u) u $ by definition, so that if \\(G\\) is bounded then \\(\\tilde{V}(x)\\) grows at most linearly. Hence, \\(V(\\bar{v}) \\to +\\infty\\) as \\(|\\bar{v}|\\to \\infty\\) so that \\(\\mathrm{e}^{-V(\\bar{v})}\\) is integrable. Thus (B.3) admits an invariant measure, which is \\[ \\rho(\\bar{v}) = \\frac{1}{Z}\\mathrm{e}^{-V(\\bar{v})}, \\] where \\(Z\\) is a normalising constant that we neglect from now on. Note that \\(\\tilde{V}(\\bar{v})\\) is an even function as \\(G(\\bar{v})\\) is odd. This implies that \\[ V(\\bar{v})=\\frac{\\bar{v}^2}{2}+ \\tilde{V}(\\bar{v}) \\] is even, and so \\[ \\int_{\\mathbb{R}} \\bar{v}\\mathrm{e}^{-V(\\bar{v})} \\mathrm{d}\\bar{v} = 0. \\] That is, as \\(t\\to \\infty\\), \\(\\mathbb{E}[\\bar{v}_t] \\to 0\\). "],
["app-ergavg.html", "C Forms of Convergence Convergence in Law Convergence of Ergodic Averages", " C Forms of Convergence In the online simulations, there are four histograms plotted: two in velocity and two in position. The left column shows a histogram calculated at time \\(t\\), whereas the right column shows a histogram calculated on \\([0,t]\\). Here we briefly explain the difference, and why both are of interest. Convergence in Law The histograms at time \\(t\\) are so that we can assess whether the system is converging in law. Recall that a stochastic process \\(X_t\\in \\mathbb{R}^n\\) converges in law to \\(X\\in\\mathbb{R}^n\\) if \\[ \\lim_{t\\to\\infty} \\mathbb{P}(X_t \\in A) = \\mathbb{P}(X\\in A), \\] for a measurable subset \\(A\\in \\mathbb{R}^n\\). If the particle system is converging in law, after a long simulation time we should see that the histogram produced looks very similar to our predicted invariant distribution: \\(\\mu_{\\pm}\\). Convergence of Ergodic Averages If instead we take a histogram on the interval \\([0,t]\\), we are assessing converge of ergodic average. This can be described intuitively as the average over time converging to the average in space, or more formally, for a process \\(X_t\\in\\mathbb{R}^n\\), \\[ \\frac{1}{t}\\int_0^t X(s)\\mathrm{d}s \\to \\mathbb{E}[X_t] \\qquad \\text{ as } t\\to\\infty. \\] By taking the histogram over the interval, we are calculating a discrete version of left hand side of this limit. The difference is important in the present of periodic behaviours. Consider the deterministic particle system, which after a long simulation, consists of one large cluster of particles moving around the torus. This system has not converged in law to the uniform distribution in position. If we observe the particles at any given time, the distribution will be far from uniform. If instead however we compute the space average by calculating the histogram on the interval, we will see and almost uniform distribution (if the cluster is moving at constant speed). Hence there is an important distinction to be made between different forms of convergence for the particle system. "],
["app-implementation.html", "D Implementation of the Particle System v0.1 v0.2 v0.3", " D Implementation of the Particle System The majority of my work this year has been spent implementing these particle systems in Python. They are available online: https://github.com/Tom271/InteractingParticleSystems/releases There are currently three versions, and in this section each will be briefly described. Throughout the three versions the algorithm for solving the system consists of the Euler-Maruyama method. When simulating particle systems, a good rule of thumb is to choose the step size of the scheme such that 10 interactions can be resolved per particle collision. For example, if a particle is moving with velocity 1 and \\(\\gamma=0.1\\), one should choose \\(\\Delta t \\approx 0.01\\). The difficulty arises in the high dimensionality of the system and simulating for long times. v0.1 The first version developed was a basic implementation with limited functionality. It was able to simulate the full system, but it was difficult to run multiple experiments automatically. This prompted the development of an automated process to allow for easier computation. A custom plotting package was also developed to produce animations of the system on the torus. v0.2 Version 0.2 moved towards an object-oriented approach for increased readability. It also introduced processing.py, a file for automated experiments. This meant we could pass a range of parameters to be tested. Then the list would be stored, and all subsets of the parameters would be evolved in the particle system. The position and velocity data is then stored with a random filename, while the parameters and file name is stored in separate YAML file. Then, when running any analysis, the YAML file is searched for matching parameters. The file name is then retrieved which then automatically loads in the trajectory data ready for analysis. This allowed for the automatic execution of 100 systems at once, such as those used to produce the heatmaps such as and the average velocity plots such as . However, this code was rather slow and often took a few days to run. v0.3 In version 0.3 (the current iteration), we moved away from an object-oriented style. This was so that we could exploit Numba, a just-in-time (JIT) compiler to massively increase computation speed [20]. Implementing a JIT compiler allows for tests that previously took days to run in less than an hour. When interested in long time dynamics, this proves very useful! "]
]
